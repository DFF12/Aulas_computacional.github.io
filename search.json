[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estatística Computacional",
    "section": "",
    "text": "1 Introdução: Problemas de interesse\nEncontrar soluções de equações não lineares onde não é possível obter uma solução analítica;\nObter integrais que apresentam uma forma complicada que inviabiliza encontrar uma solução analítica;\nGerar artificialmente amostras a partir de modelos estatísticos;\nAplicar a metodologia estudada na resolução de problemas de inferência.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdução: Problemas de interesse</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "motivacao.html",
    "href": "motivacao.html",
    "title": "Solução Numérica de Equações",
    "section": "",
    "text": "Motivação - O Estimador de Máxima Verossimilhança\nNo que segue o termo densidade, significa ou uma densidade de probabilidade (caso absolutamente contínuo) ou uma função de probabilidade (caso discreto).\nSejam \\(X_1, \\  \\dots, \\ X_n \\overset{iid}{\\sim} f(.|\\theta), \\ \\theta \\in \\ \\Theta\\), onde \\(f(.|\\theta)\\) é uma densidade, \\(\\theta\\) é um parâmetro que desejamos estimar e \\(\\Theta\\) é o espaço paramétrico;\nSuponha que observamos os valores \\(x_1, \\ \\dots, \\ x_n\\). A função de verossimilhança é definida por:\n\\[ L(\\theta) = \\prod_{i=1}^{n} f(x_i|\\theta), \\ \\theta \\in \\Theta \\]\nA função de log-verossimilhança é dada por:\n\\[ l(\\theta) = logL(\\theta) = \\sum_{i=1}^{n} log f(x_i|\\theta), \\ \\theta \\ \\in \\Theta\\]\nSeja \\(\\hat{\\theta} \\ \\in \\Theta\\) um valor do parâmetro que maximiza a função de verossimilhança, ou seja, tal que\n\\[L(\\hat{\\theta}) \\ \\geq L(\\hat{\\theta}), \\ \\text{para todo} \\ \\theta \\ \\in \\ \\Theta\\]\nEntão dizemos que \\(\\hat{\\theta}\\) é uma estimativa de máxima verossimilhança de \\(\\theta\\).\nA interpretação no caso discreto: é mais provável que \\(\\hat{\\theta}\\) tenha gerado os dados \\(x_1, \\  \\dots, \\ x_n\\)\nComo \\(\\hat{\\theta}\\) depende da amostra, escrevemos \\(\\hat{\\theta}(x_1, \\ \\dots, x_n)\\). Neste caso, \\(\\hat{\\theta}(X_1, \\ \\dots, \\ X_n)\\) é o estimador de máxima verossimilhança (EMV).\nAssim, em muitos casos encontrados na prática, encontrar o EMV é um problema relacionado a encontrar soluções em \\(\\theta\\) para a equação \\(L'(\\hat{\\theta}) = 0\\) ou \\(l'(\\theta) = 0\\).\nPor exemplo, considere uma amostra aleatória \\(X_1, \\dots, X_n\\) proveniente de uma distribuição \\(exp(\\theta)\\). Assim, cada \\(X_i\\) tem densidade de probabilidade.\n\\[\n    f(x|\\theta) = \\left\\{\n    \\begin{matrix}\n        \\theta \\exp(-\\theta x), &  \\mbox{se} & x&gt;0\\\\\n          0, &  \\mbox{se} & x\\leq 0.\n    \\end{matrix}\n         \\right.\n\\]\nO espaço paramétrico é \\(\\Theta = (0, \\infty)\\). Temos então a seguinte função de log-verassimilhança: \\[\n\\ell(\\theta)= n \\log \\theta -\\theta \\sum_{i=1}^n x_i, \\,\\,\\, \\theta &gt; 0,\n\\]\nimplicando em\n\\[\n\\ell'(\\theta)= n/\\theta - \\sum_{i=1}^n x_i, \\,\\,\\, \\theta &gt; 0.\n\\]\nA solução da equação \\(\\ell'(\\theta)=0\\) é dada por \\(\\widehat{\\theta}= n/\\sum_{i=1}^n x_i\\)\ntheta = 5\namostra = rexp(100, rate = theta); amostra\n\n  [1] 0.528411980 0.324714451 0.070575425 0.231873705 0.432811559 0.361305661\n  [7] 0.255296433 0.773969017 0.002596110 0.098746255 0.237363875 0.011054050\n [13] 0.032642319 0.124384233 0.260877223 0.119289736 0.111989385 0.044516365\n [19] 0.108541056 0.036324517 0.012416958 0.116719973 0.337881139 0.402267048\n [25] 0.125210265 0.024548061 0.221162772 0.058934654 0.115305217 0.130162705\n [31] 0.023849731 0.081540297 0.159643850 0.238135482 0.035392510 0.071502209\n [37] 0.089673397 0.014887317 0.099085513 0.076469567 0.018754744 0.214042675\n [43] 0.542940776 0.028780798 0.411526393 0.480205579 0.367570150 0.464433631\n [49] 0.413044420 0.116656352 0.291190087 0.263082761 0.283071874 0.055516604\n [55] 0.452802746 0.043800489 0.291225165 0.205043856 0.073415800 0.216899549\n [61] 0.137659974 0.142686544 0.562723444 0.326652586 0.571939178 0.186112697\n [67] 0.405928989 0.779997389 0.049889519 0.043859450 0.351478428 0.369499550\n [73] 0.105570475 0.260275520 0.123222827 0.165817353 0.939212636 0.071182862\n [79] 0.217824237 0.002852752 0.262478603 0.087498881 0.098113328 0.041044228\n [85] 0.564448571 0.203575277 0.179250961 0.120348928 0.305080247 0.053266382\n [91] 0.477505140 0.141673979 0.285881407 0.146047013 0.080755633 0.114764875\n [97] 0.067367335 0.178268025 0.036796398 0.073228765\n\nloglik &lt;- function(theta = NULL) {\n    return(length(amostra)*log(theta) - theta*sum(amostra))\n}\n\nscore_exp = function(theta = NULL) {\n    n = length(amostra)\n    \n    return(n/theta - sum(amostra))\n} \n\ntheta_x &lt;- seq(from = 1, to = 10, by = 0.1)  # Sequência de valores de -10 a 10 com incremento de 0.1\n\ny &lt;- loglik(theta = theta_x); y\n\n [1] -21.167857 -13.753625  -7.169273  -1.281787   4.012224   8.794726\n [7]  13.131792  17.077468  20.676524  23.966461  26.979004  29.741235\n[13]  32.276451  34.604842  36.744017  38.709431  40.514717  42.171964\n[19]  43.691943  45.084289  46.357658  47.519855  48.577939  49.538319\n[25]  50.406830  51.188798  51.889100  52.512212  53.062251  53.543014\n[31]  53.958009  54.310484  54.603454  54.839718  55.021884  55.152384\n[37]  55.233489  55.267324  55.255879  55.201022  55.104507  54.967984\n[43]  54.793007  54.581041  54.333468  54.051597  53.736661  53.389833\n[49]  53.012222  52.604880  52.168806  51.704950  51.214217  50.697465\n[55]  50.155515  49.589148  48.999110  48.386112  47.750835  47.093929\n[61]  46.416017  45.717695  44.999533  44.262080  43.505859  42.731376\n[67]  41.939113  41.129535  40.303090  39.460207  38.601299  37.726766\n[73]  36.836989  35.932340  35.013173  34.079833  33.132651  32.171948\n[79]  31.198032  30.211202  29.211746  28.199944  27.176065  26.140371\n[85]  25.093115  24.034540  22.964884  21.884377  20.793241  19.691693\n[91]  18.579941\n\nmaximo_estimado = optimise(loglik, interval = c(1, 10), maximum = T)\ny_score = score_exp(theta = theta_x)\nFigura 1: log-verossimilhança1\n\n\n\n\n\n\n\n\n\n\n\nFigura 2: Função score\nA função \\(S(\\theta)=\\ell'(\\theta)\\), \\(\\theta \\in \\Theta\\), é denominada função escore. Assim, em geral, encontrar o EMV é equivalente a resolver em \\(\\theta\\) a equação \\(S(\\theta)=0\\).\nNo exemplo acima obtivemos uma solução analítica para esta equação. Mas em algumas situações isto não é possível.\nConsidere o seguinte exemplo (Bolfarine e Sandoval, 2010): sejam \\(X_1, \\ldots,X_n \\overset{iid}{\\sim} f(.|\\theta)\\), onde\n\\[\n    f(x|\\theta) = \\left\\{\n    \\begin{matrix}\n        \\frac{1}{2}(1+\\theta x), &  \\mbox{se} & x&gt;0\\\\\n          0, &  \\mbox{se} & x\\leq 0.\n    \\end{matrix}\n         \\right.\n\\tag{1}\\]\nOnde \\(-1 &lt; \\theta &lt;1\\) verifique que \\(f(\\cdot|\\theta)\\) é uma densidade.\nEntão é possível mostrar que \\[\nS(\\theta)= \\sum_{i=1}^n \\frac{x_i}{1+\\theta x_i}.\n\\]\nNo entanto, não há solução analítica para \\(S(\\theta)=0\\). Os gráficos a seguir mostram o comportamento das funções de log-verossimilhança (à esquerda) e escore (à direita). A amostra utilizada foi gerada artificialmente a partir do modelo (Equação 1), com \\(n=40\\) e \\(\\theta=-0,35\\) (como gerar esta amostra? voltaremos em breve a este assunto).",
    "crumbs": [
      "Solução Numérica de Equações"
    ]
  },
  {
    "objectID": "motivacao.html#motivação---o-estimador-de-máxima-verossimilhança",
    "href": "motivacao.html#motivação---o-estimador-de-máxima-verossimilhança",
    "title": "Solução Numérica de Equações",
    "section": "",
    "text": "Nota\n\n\n\nPara cada amostra observada \\(\\textbf{x} = \\ (x_1, \\ \\dots, x_n)\\)\nA definição nos diz que \\(\\hat{\\theta}(\\textbf{x})\\) é um ponto de máximo global. Podemos ter nenhum ou mais de um máximo global\nSuponha que \\(\\Theta\\) é um intervalo e que o ponto \\(\\hat{\\theta}\\) é um ponto interior de \\(\\Theta\\) que é ponto de máximo de L, podendo ser um máximo local. Se L tem derivada em \\(\\hat{\\theta}\\), então \\(L'( \\hat{\\theta}) = 0\\). Ou seja, \\(\\hat{\\theta}\\) é um ponto estacionário de L (também dizemos \\(\\hat{\\theta}\\) é um zero da função L’). Este resultado é conhecido no Cálculo como Teorema de Fermat para Pontos Estacionários.\nOu seja, sob as condições acima, se \\(\\hat{\\theta}\\) for EMV, então a derivada de L se anula neste ponto. A recíproca pode não ser verdadeira.\n\n\n\n\n\n\n\n\n\n\n#Plotando a log-verossimilhança\nplot(theta_x, y, type = \"l\", col = \"blue\", lwd = 2,\n     main = paste(\"Gráfico da log-verossimilhança de: f(x) =\", theta, \"exp(-\", theta, \"x)\"),\n     xlab = \"x\", ylab = \"f(x)\")\nabline(v=theta, col=\"blue\")\nabline(v=maximo_estimado, col=\"red\")\n\n#Plotando a função score\nplot(theta_x, y_score, type = \"l\", col = \"blue\", lwd = 2,\n     main = paste(\"Gráfico da funcao score de: f(x) =\", theta, \"exp(-\", theta, \"x)\"),\n     xlab = \"x\", ylab = \"f(x)\")\nabline(h=0, col=\"red\")\nabline(v=maximo_estimado, col=\"red\")",
    "crumbs": [
      "Solução Numérica de Equações"
    ]
  },
  {
    "objectID": "metodo_bissecao.html",
    "href": "metodo_bissecao.html",
    "title": "3  Método da Bisseção",
    "section": "",
    "text": "asdajhduahd",
    "crumbs": [
      "Solução Numérica de Equações",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Método da Bisseção</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  }
]