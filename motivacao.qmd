# Solução Numérica de Equações

## Motivação - O Estimador de Máxima Verossimilhança

No que segue o termo densidade, significa ou uma densidade de probabilidade (caso absolutamente contínuo) ou uma função de probabilidade (caso discreto).

Sejam $X_1, \  \dots, \ X_n \overset{iid}{\sim} f(.|\theta), \ \theta \in \ \Theta$, onde $f(.|\theta)$ é uma densidade, $\theta$ é um parâmetro que desejamos estimar e $\Theta$ é o espaço paramétrico;

Suponha que observamos os valores $x_1, \ \dots, \ x_n$. A função de verossimilhança é definida por:

$$ L(\theta) = \prod_{i=1}^{n} f(x_i|\theta), \ \theta \in \Theta $$

A função de log-verossimilhança é dada por:

$$ l(\theta) = logL(\theta) = \sum_{i=1}^{n} log f(x_i|\theta), \ \theta \ \in \Theta$$


Seja $\hat{\theta} \ \in \Theta$ um valor do parâmetro que maximiza a função de verossimilhança, ou seja, tal que

$$L(\hat{\theta}) \ \geq L(\hat{\theta}), \ \text{para todo} \ \theta \ \in \ \Theta$$

Então dizemos que $\hat{\theta}$ é uma estimativa de máxima verossimilhança de $\theta$.

A interpretação no caso discreto: é mais provável que $\hat{\theta}$ tenha gerado os dados $x_1, \  \dots, \ x_n$

Como $\hat{\theta}$ depende da amostra, escrevemos $\hat{\theta}(x_1, \ \dots, x_n)$. Neste caso, $\hat{\theta}(X_1, \ \dots, \ X_n)$ é o estimador de máxima verossimilhança (EMV).

:::{.callout-note}
 Para cada amostra observada $\textbf{x} = \ (x_1, \ \dots, x_n)$ 

 A definição nos diz que $\hat{\theta}(\textbf{x})$ é um ponto de máximo global. Podemos ter nenhum ou mais de um máximo global

 Suponha que $\Theta$ é um intervalo e que o ponto $\hat{\theta}$ é um ponto interior de $\Theta$ que é ponto de máximo de L, podendo ser um máximo local. Se L tem derivada em $\hat{\theta}$, então $L'( \hat{\theta}) = 0$. Ou seja, $\hat{\theta}$ é um ponto estacionário de L (também dizemos $\hat{\theta}$ é um zero da função L'). Este resultado é conhecido no Cálculo como Teorema de Fermat para Pontos Estacionários.

 Ou seja, sob as condições acima, se $\hat{\theta}$ for EMV, então a derivada de L se anula neste ponto. A recíproca pode não ser verdadeira.
:::

Assim, em muitos casos encontrados na prática, encontrar o EMV é um problema relacionado a encontrar soluções em $\theta$ para a equação $L'(\hat{\theta}) = 0$ ou $l'(\theta) = 0$.

Por exemplo, considere uma amostra aleatória $X_1, \dots, X_n$ proveniente de uma distribuição $exp(\theta)$. Assim, cada $X_i$ tem densidade de probabilidade.


$$
    f(x|\theta) = \left\{
	\begin{matrix}
	    \theta \exp(-\theta x), &  \mbox{se} & x>0\\
		  0, &  \mbox{se} & x\leq 0. 
	\end{matrix}
		 \right.
$$

O espaço paramétrico é $\Theta = (0, \infty)$. Temos então a seguinte função de log-verassimilhança:
$$
\ell(\theta)= n \log \theta -\theta \sum_{i=1}^n x_i, \,\,\, \theta > 0, 
$$

implicando em

$$
\ell'(\theta)= n/\theta - \sum_{i=1}^n x_i, \,\,\, \theta > 0. 
$$

A solução da equação $\ell'(\theta)=0$ é dada por $\widehat{\theta}= n/\sum_{i=1}^n x_i$ 

A função  $S(\theta)=\ell'(\theta)$, $\theta \in \Theta$,  é denominada função escore.  Assim, em geral, encontrar o EMV é equivalente a resolver em $\theta$ a equação $S(\theta)=0$.  

No exemplo acima obtivemos uma solução analítica para esta equação. Mas em  algumas situações isto não é possível.

Considere o seguinte exemplo (Bolfarine e Sandoval, 2010): sejam $X_1, \ldots,X_n \overset{iid}{\sim} f(.|\theta)$, onde 

$$
    f(x|\theta) = \left\{
	\begin{matrix}
	    \frac{1}{2}(1+\theta x), &  \mbox{se} & x>0\\
		  0, &  \mbox{se} & x\leq 0. 
	\end{matrix}
		 \right.
$$ {#eq-monica-score}

Onde $-1 < \theta <1$ verifique que $f(\cdot|\theta)$ é uma densidade.

Então é possível mostrar que
$$
S(\theta)= \sum_{i=1}^n \frac{x_i}{1+\theta x_i}. 
$$

No entanto, não há solução analítica para $S(\theta)=0$. 
Os gráficos a seguir mostram o comportamento das funções de log-verossimilhança (à esquerda) e escore (à direita). A amostra utilizada foi gerada artificialmente a partir do modelo  (@eq-monica-score), com $n=40$ e $\theta=-0,35$ (como gerar esta amostra? voltaremos em breve a este assunto). 